{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "ghC5RCwBHRQy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained model specifically fine-tuned for NER tasks\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBL06uwJ-0uW",
        "outputId": "82b97c7a-f710-406d-e00b-335717b57475"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a function to analyze the text for named entities\n",
        "def analyze_text(text):\n",
        "    \"\"\"\n",
        "    This function takes a text input, analyzes it for named entities,\n",
        "    and displays images for any recognized persons.\n",
        "    \"\"\"\n",
        "    entities = ner_pipeline(text)  # Use the NER pipeline to process the input text\n",
        "\n",
        "    recognized_entities = {}  # Initialize a dictionary to hold recognized person entities\n",
        "\n",
        "    for entity in entities:\n",
        "        name = entity['word']  # The recognized entity (e.g., a person's name)\n",
        "        label = entity['entity_group']  # The type of entity (e.g., 'PER' for person)\n",
        "        score = entity['score']  # Confidence score for the recognized entity\n",
        "\n",
        "        if label == 'PER':  # Only process entities labeled as 'PER' (persons)\n",
        "            recognized_entities[name] = score  # Add recognized name and score to the dictionary\n",
        "            print(f\"Entity: {name}, Label: {label}, Score: {score:.2f}\")  # Print entity details\n",
        ""
      ],
      "metadata": {
        "id": "ws6otbjp_M3f"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Apple is looking at buying U.K. startup for $1 billion. The CEO of Microsoft, Satya Nadella, visited Apple.\"\n",
        "analyze_text(text)  # Call the function with the example text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJjQs94y_ZoK",
        "outputId": "8205090f-85be-4408-997e-e76dbd42dcd1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Satya Nadella, Label: PER, Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Martin Scorsese is known for directing classic films such as Goodfellas and The Irishman. His unique storytelling style and ability to portray complex characters have earned him numerous accolades, including Academy Awards. Quentin Tarantino, another acclaimed director, is famous for films like Pulp Fiction and Inglourious Basterds.\"\n",
        "analyze_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK8Z-Q7lD5Ei",
        "outputId": "e84bef95-8f15-4ea6-c522-45b8371c079d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Martin Scorsese, Label: PER, Score: 0.99\n",
            "Entity: Quentin Tarantino, Label: PER, Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. dbmdz/bert-large-cased-finetuned-conll03-english: Fine-tuned on the CoNLL-03 dataset.\n",
        "2. dbmdz/bert-base-cased-finetuned-conll03-english: A smaller version of the above model.\n",
        "3. huggingface/transformers: A generic model that supports multiple tasks, including NER."
      ],
      "metadata": {
        "id": "ufDyxS3nG3kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the pre-trained NER model\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
        "\n",
        "def recognize_entities(text):\n",
        "    \"\"\"\n",
        "    Function to recognize entities in the provided text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The input text for entity recognition.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of recognized entities with their labels and scores.\n",
        "    \"\"\"\n",
        "    entities = ner_pipeline(text)\n",
        "    return [(entity['word'], entity['entity'], entity['score']) for entity in entities]  # Updated key here\n",
        "\n",
        "# Example usage\n",
        "text_input = \"Barack Obama was born in Hawaii. He was elected president in 2008.\"\n",
        "results = recognize_entities(text_input)\n",
        "\n",
        "# Display results\n",
        "for entity, label, score in results:\n",
        "    print(f\"Entity: {entity}, Label: {label}, Score: {score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ta-WTktG37I",
        "outputId": "a95f2ad1-2633-4077-ad29-beaa0a707f1c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Barack, Label: B-PER, Score: 1.00\n",
            "Entity: Obama, Label: I-PER, Score: 1.00\n",
            "Entity: Hawaii, Label: B-LOC, Score: 1.00\n"
          ]
        }
      ]
    }
  ]
}